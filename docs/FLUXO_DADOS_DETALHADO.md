# ğŸ“Š Fluxo de Dados Detalhado - Sistema RAG

**Atualizado:** 04/12/2025  
**VersÃ£o:** 2.0  
**PÃºblico:** Desenvolvedores e Administradores

---

## ğŸ¯ VisÃ£o Geral

Este documento descreve em detalhes tÃ©cnicos o fluxo completo de processamento de dados no sistema, desde o upload atÃ© a disponibilizaÃ§Ã£o para busca semÃ¢ntica. Use este guia para entender a arquitetura, diagnosticar problemas e fazer manutenÃ§Ãµes.

---

## ğŸ“‹ SumÃ¡rio

1. [Arquitetura Geral](#arquitetura-geral)
2. [Fluxo Passo a Passo](#fluxo-passo-a-passo)
3. [Tabelas do Banco de Dados](#tabelas-do-banco-de-dados)
4. [Schemas e Templates](#schemas-e-templates)
5. [Como Intervir e Debugar](#como-intervir-e-debugar)
6. [Exemplos PrÃ¡ticos](#exemplos-prÃ¡ticos)
7. [Troubleshooting](#troubleshooting)

---

## ğŸ—ï¸ Arquitetura Geral

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚   (Next.js)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Routes    â”‚
â”‚   (Next.js)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Upload â”‚    â”‚  RAG   â”‚    â”‚   AI   â”‚    â”‚Storage â”‚
    â”‚Service â”‚    â”‚Processorâ”‚    â”‚Service â”‚    â”‚Service â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   NeonDB      â”‚
                        â”‚  (PostgreSQL  â”‚
                        â”‚  + pgvector)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tecnologias

- **Frontend**: Next.js 14 (App Router), React, TailwindCSS
- **Backend**: Next.js API Routes, TypeScript
- **Banco de Dados**: NeonDB (PostgreSQL 16 + pgvector)
- **IA**: OpenAI (GPT-4, text-embedding-3-small)
- **ORM**: Drizzle ORM
- **Storage**: Sistema de arquivos local (`public/uploads`)

---

## ğŸ”„ Fluxo Passo a Passo

### Etapa 1: Upload do Arquivo

**Endpoint:** `POST /api/documents/upload`

**Processo:**
1. UsuÃ¡rio seleciona arquivo(s) na interface
2. ValidaÃ§Ã£o client-side (tipo, tamanho)
3. Arquivo enviado via `FormData`
4. Servidor recebe e valida
5. Calcula hash SHA-256 do conteÃºdo
6. Cria diretÃ³rios: `public/uploads/{org}/{year}/{month}/`
7. Salva arquivo: `{hash}-{filename}`
8. Cria registro na tabela `documents` com status `pending`

**CÃ³digo Relevante:**
```typescript
// app/api/documents/upload/route.ts
const hash = await calculateFileHash(file)
const uploadPath = getUploadPath(organizationId, file.name, hash)
await writeFile(fullPath, buffer)

const [doc] = await db.insert(documents).values({
  organizationId,
  uploadedBy: session.user.id,
  fileName,
  filePath: uploadPath,
  fileHash: hash,
  status: 'pending',
}).returning()
```

**Tabelas Afetadas:** `documents`

---

### Etapa 2: ConversÃ£o para Markdown

**FunÃ§Ã£o:** `convertDocument()` em `lib/services/document-converter.ts`

**Processo:**
1. Detecta tipo do arquivo (PDF, DOCX, DOC, TXT)
2. Aplica conversor apropriado:
   - **DOCX**: `mammoth` (preserva estrutura)
   - **PDF**: `pdf-parse` (extraÃ§Ã£o nativa) ou Pandoc (fallback)
   - **DOC**: `textract` ou LibreOfficeâ†’DOCXâ†’mammoth
   - **TXT**: leitura direta
3. Opcionalmente usa Google Gemini para estruturar melhor (se `GOOGLE_GENERATIVE_AI_API_KEY` configurada)
4. Limpa e normaliza Markdown
5. Conta palavras
6. Salva markdown temporariamente

**CÃ³digo Relevante:**
```typescript
// lib/services/document-converter.ts
export async function convertDocument(filePath: string): Promise<{
  markdown: string
  wordCount: number
}> {
  const ext = path.extname(filePath).toLowerCase()
  
  if (ext === '.docx') {
    return convertDocx(filePath)
  } else if (ext === '.pdf') {
    return convertPdf(filePath)
  }
  // ... outros formatos
}
```

**SaÃ­da:**
- Markdown limpo e estruturado
- Contagem de palavras
- Arquivo temporÃ¡rio em `.rag-tmp/{hash}.md`

---

### Etapa 3: Filtragem por Tamanho

**ValidaÃ§Ãµes:**
- **MÃ­nimo:** 300 palavras (`MIN_WORDS`)
- **MÃ¡ximo:** 1.000.000 palavras (`MAX_WORDS`)

**Processo:**
```typescript
if (wordCount < MIN_WORDS) {
  await markFileRejected(path, `Muito pequeno: ${wordCount} palavras`)
  return { success: false, error: 'Documento muito pequeno' }
}

if (wordCount > MAX_WORDS) {
  await markFileRejected(path, `Muito grande: ${wordCount} palavras`)
  return { success: false, error: 'Documento muito grande' }
}
```

**Tabelas Afetadas:** `document_files` (status â†’ `rejected`)

---

### Etapa 4: ClassificaÃ§Ã£o com IA

**FunÃ§Ã£o:** `classifyDocument()` em `lib/services/classifier.ts`

**Processo:**
1. Carrega configuraÃ§Ã£o de classificaÃ§Ã£o ativa
2. Monta prompt com schema desejado
3. Envia para GPT-4 (ou modelo configurado)
4. IA retorna JSON estruturado com metadados:
   - Tipo de documento (ex: contrato, petiÃ§Ã£o)
   - Ãrea jurÃ­dica (ex: direito civil)
   - Partes envolvidas
   - Datas importantes
   - Resumo
   - Outros campos customizados
5. Valida resposta contra schema
6. Cria `TemplateDocument`
7. Salva na tabela `templates`

**CÃ³digo Relevante:**
```typescript
// lib/services/classifier.ts
const response = await generateText({
  model: openai(configModel),
  messages: [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userPrompt }
  ],
  temperature: 0.1,
})

const classification = JSON.parse(response.text)
const templateDoc = createTemplateDocument(classification, markdown, fileId)
const templateId = await storeTemplate(templateDoc, fileId)
```

**Tabelas Afetadas:** 
- `templates` (novo registro)
- `document_files` (atualizaÃ§Ã£o de metadados)

**Custos de IA:**
- Input: ~1000-10000 tokens (dependendo do tamanho do documento)
- Output: ~500-2000 tokens
- Modelo padrÃ£o: GPT-4

---

### Etapa 5: GeraÃ§Ã£o de Chunks

**FunÃ§Ã£o:** `chunkMarkdown()` em `lib/services/chunker.ts`

**Processo:**
1. Divide documento em seÃ§Ãµes lÃ³gicas
2. Respeita limites de tokens (`MAX_TOKENS=800`)
3. Evita quebrar no meio de parÃ¡grafos
4. MantÃ©m contexto (headers sÃ£o incluÃ­dos em chunks filhos)
5. Gera array de chunks com metadados:
   - `content`: texto do chunk
   - `chunkIndex`: posiÃ§Ã£o no documento
   - `tokenCount`: nÃºmero de tokens estimado
   - `startLine`, `endLine`: localizaÃ§Ã£o no markdown original

**CÃ³digo Relevante:**
```typescript
// lib/services/chunker.ts
export function chunkMarkdown(
  markdown: string,
  maxTokens: number = 800
): ChunkData[] {
  const lines = markdown.split('\n')
  const chunks: ChunkData[] = []
  let currentChunk: string[] = []
  let tokenCount = 0

  for (const line of lines) {
    const lineTokens = estimateTokens(line)
    
    if (tokenCount + lineTokens > maxTokens && currentChunk.length > 0) {
      chunks.push(createChunk(currentChunk, chunks.length))
      currentChunk = []
      tokenCount = 0
    }
    
    currentChunk.push(line)
    tokenCount += lineTokens
  }
  
  return chunks
}
```

**SaÃ­da:**
- Array de 10-500 chunks (dependendo do tamanho do documento)
- Cada chunk tem 200-800 tokens

---

### Etapa 6: GeraÃ§Ã£o de Embeddings

**FunÃ§Ã£o:** `generateEmbeddings()` em `lib/services/embedding-generator.ts`

**Processo:**
1. Agrupa chunks em lotes (`BATCH_SIZE=64`)
2. Para cada lote:
   - Envia para OpenAI `text-embedding-3-small`
   - Recebe vetores de 1536 dimensÃµes
   - Armazena em array
3. Retorna embeddings na mesma ordem dos chunks

**CÃ³digo Relevante:**
```typescript
// lib/services/embedding-generator.ts
export async function generateEmbeddings(
  texts: string[],
  batchSize: number = 64,
  templateId: string
): Promise<EmbeddingResult[]> {
  const results: EmbeddingResult[] = []
  
  for (let i = 0; i < texts.length; i += batchSize) {
    const batch = texts.slice(i, i + batchSize)
    
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: batch,
    })
    
    response.data.forEach((item, idx) => {
      results.push({
        embedding: item.embedding, // [1536 dimensÃµes]
        index: i + idx,
      })
    })
  }
  
  return results
}
```

**Custos de IA:**
- ~$0.00002 por 1000 tokens
- Documento de 50 pÃ¡ginas â‰ˆ 200 chunks â‰ˆ $0.004

**SaÃ­da:**
- Array de vetores de 1536 dimensÃµes (float32)

---

### Etapa 7: Armazenamento no Banco

**FunÃ§Ã£o:** `storeChunks()` em `lib/services/store-embeddings.ts`

**Processo:**
1. Para cada chunk + embedding:
   - Cria registro em `template_chunks`
   - Armazena conteÃºdo, metadados e vetor
   - pgvector indexa automaticamente para busca
2. Atualiza `documents.status` â†’ `completed`
3. Define `documents.processedAt` â†’ agora
4. Atualiza contadores (`totalChunks`, `totalTokens`)

**CÃ³digo Relevante:**
```typescript
// lib/services/store-embeddings.ts
export async function storeChunks(
  templateId: string,
  chunks: ChunkWithEmbedding[]
) {
  const values = chunks.map((chunk, index) => ({
    templateId,
    chunkIndex: index,
    content: chunk.content,
    embedding: JSON.stringify(chunk.embedding), // pgvector
    tokenCount: chunk.tokenCount,
    startLine: chunk.startLine,
    endLine: chunk.endLine,
  }))
  
  await db.insert(templateChunks).values(values)
}
```

**Tabelas Afetadas:**
- `template_chunks` (bulk insert)
- `documents` (status update)
- `document_files` (metadata update)

---

### Etapa 8: Busca SemÃ¢ntica

**DisponÃ­vel apÃ³s Etapa 7**

**Como Funciona:**
1. UsuÃ¡rio faz pergunta no chat
2. Pergunta Ã© transformada em embedding (mesmo modelo)
3. pgvector calcula similaridade de cosseno entre embedding da pergunta e embeddings dos chunks
4. Retorna top K chunks mais similares (ex: K=5)
5. Chunks sÃ£o usados como contexto para GPT-4 responder

**CÃ³digo Relevante:**
```typescript
// lib/services/rag-search.ts
const queryEmbedding = await generateEmbedding(query)

const results = await db
  .select()
  .from(templateChunks)
  .where(
    sql`(embedding <=> ${JSON.stringify(queryEmbedding)}) < ${threshold}`
  )
  .orderBy(sql`embedding <=> ${JSON.stringify(queryEmbedding)}`)
  .limit(limit)
```

---

## ğŸ—„ï¸ Tabelas do Banco de Dados

### 1. `documents` - Registro de Upload

**PropÃ³sito:** Rastreia arquivos uploadados e status de processamento.

**Campos Principais:**
```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY,
  organization_id UUID NOT NULL,
  uploaded_by UUID NOT NULL,
  
  file_name TEXT NOT NULL,
  file_path TEXT NOT NULL,
  file_hash TEXT NOT NULL,
  document_type TEXT NOT NULL, -- pdf, docx, doc, txt
  
  status TEXT NOT NULL DEFAULT 'pending', -- pending, processing, completed, failed
  error_message TEXT,
  processed_at TIMESTAMP,
  
  total_chunks INTEGER DEFAULT 0,
  total_tokens INTEGER DEFAULT 0,
  
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

**Ãndices:**
- `organization_id` (para filtro por organizaÃ§Ã£o)
- `status` (para buscar pendentes/failed)
- `file_hash` (para detectar duplicatas)

---

### 2. `document_files` - Arquivo Processado (RAG)

**PropÃ³sito:** Criado durante processamento RAG. Armazena metadados do arquivo convertido.

**Campos Principais:**
```sql
CREATE TABLE document_files (
  id UUID PRIMARY KEY,
  organization_id UUID,
  created_by UUID,
  
  file_path TEXT NOT NULL UNIQUE,
  file_name TEXT NOT NULL,
  file_hash TEXT NOT NULL,
  file_type TEXT DEFAULT 'document',
  
  status TEXT NOT NULL DEFAULT 'pending', -- pending, processing, completed, rejected
  rejected_reason TEXT,
  words_count INTEGER,
  processed_at TIMESTAMP,
  
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

**RelaÃ§Ã£o com `documents`:**
- Criado quando processamento RAG inicia
- `file_path` deve corresponder ao `file_path` em `documents`
- Ambos compartilham `file_hash`

---

### 3. `templates` - Metadados ExtraÃ­dos

**PropÃ³sito:** Armazena metadados estruturados extraÃ­dos pela IA (classificaÃ§Ã£o).

**Campos Principais:**
```sql
CREATE TABLE templates (
  id UUID PRIMARY KEY,
  document_file_id UUID NOT NULL REFERENCES document_files(id),
  
  template_area TEXT, -- ex: "Direito Civil"
  template_doc_type TEXT, -- ex: "Contrato"
  template_parties JSONB, -- partes envolvidas
  template_dates JSONB, -- datas importantes
  template_summary TEXT,
  metadata JSONB, -- outros campos customizados
  
  model_provider TEXT, -- ex: "openai"
  model_name TEXT, -- ex: "gpt-4"
  input_tokens INTEGER,
  output_tokens INTEGER,
  cost DECIMAL,
  
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

**JSONB Fields:**
- `template_parties`: `["Empresa X", "JoÃ£o Silva"]`
- `template_dates`: `{"assinatura": "2024-01-15", "vencimento": "2025-01-15"}`
- `metadata`: campos flexÃ­veis definidos pelo schema de classificaÃ§Ã£o

---

### 4. `template_chunks` - Chunks com Embeddings

**PropÃ³sito:** Armazena pedaÃ§os do documento com vetores para busca semÃ¢ntica.

**Campos Principais:**
```sql
CREATE TABLE template_chunks (
  id UUID PRIMARY KEY,
  template_id UUID NOT NULL REFERENCES templates(id) ON DELETE CASCADE,
  
  chunk_index INTEGER NOT NULL,
  content TEXT NOT NULL,
  embedding vector(1536), -- pgvector!
  
  token_count INTEGER,
  start_line INTEGER,
  end_line INTEGER,
  
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Ãndice vetorial para busca rÃ¡pida
CREATE INDEX idx_template_chunks_embedding ON template_chunks 
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

**pgvector:**
- `vector(1536)`: tipo especial do pgvector
- `vector_cosine_ops`: usa similaridade de cosseno
- `ivfflat`: Ã­ndice aproximado (mais rÃ¡pido que busca linear)

---

## ğŸ“ Schemas e Templates

### O que Ã© um Schema/Template?

**Schema** = Estrutura de dados que define quais metadados extrair  
**Template** = InstÃ¢ncia preenchida do schema para um documento especÃ­fico

### Exemplo de Schema

```json
{
  "name": "ContratoLocacao",
  "description": "Contrato de locaÃ§Ã£o de imÃ³veis",
  "fields": [
    {
      "name": "locador",
      "type": "text",
      "description": "Nome do locador (proprietÃ¡rio)"
    },
    {
      "name": "locatario",
      "type": "text",
      "description": "Nome do locatÃ¡rio (inquilino)"
    },
    {
      "name": "endereco_imovel",
      "type": "text",
      "description": "EndereÃ§o completo do imÃ³vel"
    },
    {
      "name": "valor_aluguel",
      "type": "number",
      "description": "Valor mensal do aluguel em reais"
    },
    {
      "name": "data_inicio",
      "type": "date",
      "description": "Data de inÃ­cio do contrato"
    },
    {
      "name": "data_fim",
      "type": "date",
      "description": "Data de tÃ©rmino do contrato (se houver)"
    }
  ]
}
```

### Exemplo de Template Preenchido

```json
{
  "templateDocType": "Contrato de LocaÃ§Ã£o",
  "templateArea": "Direito Civil",
  "locador": "Maria da Silva",
  "locatario": "JoÃ£o Santos",
  "endereco_imovel": "Rua das Flores, 123, SÃ£o Paulo - SP",
  "valor_aluguel": 2500.00,
  "data_inicio": "2024-01-01",
  "data_fim": "2025-01-01"
}
```

### Como Schemas SÃ£o Usados

1. **Na ClassificaÃ§Ã£o (Etapa 4):**
   - IA recebe schema como parte do prompt
   - Extrai informaÃ§Ãµes do documento conforme schema
   - Retorna JSON estruturado

2. **No Armazenamento:**
   - Dados sÃ£o salvos em `templates.metadata` (JSONB)
   - Campos principais (area, docType) tÃªm colunas dedicadas

3. **Na Busca:**
   - Permite filtros: "buscar apenas contratos"
   - Permite queries estruturadas: "contratos com valor > R$ 2000"

---

## ğŸ”§ Como Intervir e Debugar

### Ver Logs de Processamento

**Console do servidor:**
```bash
# Logs em tempo real
tail -f .next/*.log

# Filtrar por documento especÃ­fico
grep "[UPLOAD]" .next/*.log
grep "[PROCESS uuid-123]" .next/*.log
```

**Logs estruturados:**
Procure por tags:
- `[UPLOAD]` - Upload e salvamento
- `[PROCESS]` - Processamento RAG
- `[CLASSIFY]` - ClassificaÃ§Ã£o com IA
- `[CHUNK]` - GeraÃ§Ã£o de chunks
- `[EMBED]` - GeraÃ§Ã£o de embeddings

### Reprocessar Documento Manualmente

**Via API:**
```bash
curl -X POST http://localhost:3000/api/documents/{docId}/process \
  -H "Cookie: next-auth.session-token=..." 
```

**Via Script:**
```bash
npx tsx scripts/process-pending-documents.ts
```

### Inspecionar Dados IntermediÃ¡rios

**Markdown temporÃ¡rio:**
```bash
# Ver markdown gerado
ls -la .rag-tmp/
cat .rag-tmp/{hash}.md
```

**Banco de dados:**
```sql
-- Ver documento e status
SELECT id, file_name, status, error_message, processed_at 
FROM documents 
WHERE id = 'uuid-123';

-- Ver template extraÃ­do
SELECT t.template_doc_type, t.template_area, t.metadata
FROM templates t
JOIN document_files df ON t.document_file_id = df.id
WHERE df.file_path = '/uploads/...';

-- Ver chunks
SELECT chunk_index, token_count, LEFT(content, 100) as preview
FROM template_chunks
WHERE template_id = 'template-uuid'
ORDER BY chunk_index
LIMIT 10;

-- Testar busca semÃ¢ntica
SELECT chunk_index, content, 
       embedding <=> '[0.1, 0.2, ..., 0.5]' as distance
FROM template_chunks
WHERE template_id = 'template-uuid'
ORDER BY distance
LIMIT 5;
```

### Testar Pipeline Etapa por Etapa

**1. Testar ConversÃ£o:**
```typescript
import { convertDocument } from '@/lib/services/document-converter'

const result = await convertDocument('/path/to/file.pdf')
console.log(result.markdown)
console.log('Palavras:', result.wordCount)
```

**2. Testar ClassificaÃ§Ã£o:**
```typescript
import { classifyDocument } from '@/lib/services/classifier'

const classification = await classifyDocument(markdown)
console.log(classification)
```

**3. Testar Chunking:**
```typescript
import { chunkMarkdown } from '@/lib/services/chunker'

const chunks = chunkMarkdown(markdown, 800)
console.log(`${chunks.length} chunks gerados`)
console.log('Primeiro chunk:', chunks[0])
```

**4. Testar Embeddings:**
```typescript
import { generateEmbeddings } from '@/lib/services/embedding-generator'

const embeddings = await generateEmbeddings(['texto de teste'], 1, 'test-id')
console.log('DimensÃµes:', embeddings[0].embedding.length) // 1536
```

---

## ğŸ“š Exemplos PrÃ¡ticos

### Exemplo 1: Adicionar Novo Campo ao Schema

**1. Atualizar configuraÃ§Ã£o de classificaÃ§Ã£o:**
```typescript
// Em settings/classification
{
  "fields": [
    {
      "name": "numero_processo",
      "type": "text",
      "description": "NÃºmero do processo judicial (se houver)"
    }
  ]
}
```

**2. Modelo jÃ¡ extrai automaticamente na prÃ³xima classificaÃ§Ã£o**

**3. Buscar usando novo campo:**
```sql
SELECT * FROM templates
WHERE metadata->>'numero_processo' = '1234567-89.2024.8.26.0100';
```

### Exemplo 2: Criar Schema Customizado

**Via Interface Admin:**
1. Acesse `/admin/document-schemas`
2. Clique "Novo Schema"
3. Defina campos
4. Marque como "Ativo"

**Campos aparecem automaticamente no prompt de classificaÃ§Ã£o.**

---

## ğŸš¨ Troubleshooting

### Problema: Documento fica em "pending" para sempre

**Causas possÃ­veis:**
1. Processamento nÃ£o foi iniciado
2. Erro nÃ£o capturado

**SoluÃ§Ãµes:**
```bash
# Verificar se hÃ¡ processo rodando
ps aux | grep "process-pending"

# Reprocessar manualmente
npx tsx scripts/process-pending-documents.ts

# Ver logs
grep "ERROR" .next/*.log | tail -20
```

### Problema: "Arquivo nÃ£o encontrado" mas existe no banco

**Causa:** Arquivo foi registrado mas nÃ£o salvo no disco

**SoluÃ§Ã£o:**
```sql
-- Marcar como failed
UPDATE documents
SET status = 'failed',
    error_message = 'Arquivo nÃ£o encontrado no disco'
WHERE id = 'uuid-123';
```

Depois faÃ§a novo upload.

### Problema: ClassificaÃ§Ã£o extrai dados errados

**Causa:** Prompt inadequado ou documento ambÃ­guo

**SoluÃ§Ã£o:**
1. Revisar schema de classificaÃ§Ã£o
2. Adicionar exemplos ao prompt
3. Ajustar temperatura do modelo
4. Reprocessar documento

### Problema: Busca semÃ¢ntica nÃ£o encontra documentos relevantes

**Causas:**
1. Embeddings nÃ£o foram gerados
2. Threshold de similaridade muito alto
3. Query muito genÃ©rica

**SoluÃ§Ãµes:**
```sql
-- Verificar se hÃ¡ embeddings
SELECT COUNT(*) FROM template_chunks
WHERE template_id = 'template-uuid';

-- Testar busca com threshold mais baixo
SELECT * FROM template_chunks
WHERE embedding <=> $queryEmbedding < 0.8 -- mais permissivo
LIMIT 10;
```

---

## ğŸ“ Suporte

**DÃºvidas tÃ©cnicas:** Consulte este documento primeiro  
**Bugs:** Verifique logs e tabelas antes de reportar  
**Feature requests:** Documente caso de uso e justificativa

---

**Mantido por:** Equipe de Desenvolvimento  
**Ãšltima atualizaÃ§Ã£o:** 04/12/2025

