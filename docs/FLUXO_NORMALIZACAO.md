# Fluxo de NormalizaÃ§Ã£o de Dados - QS Nexus

## VisÃ£o Geral

O sistema QS Nexus processa **3 tipos de arquivos diferentes**, cada um com seu prÃ³prio fluxo de normalizaÃ§Ã£o otimizado para o tipo de dado:

1. **Documentos (PDF/DOCX/TXT)** - Documentos gerais para RAG
2. **SPED (ECD/ECF)** - Arquivos contÃ¡beis estruturados  
3. **CSV** - Planilhas/dados tabulares

**Todos seguem o mesmo padrÃ£o base:**

```
Upload â†’ NormalizaÃ§Ã£o â†’ ClassificaÃ§Ã£o â†’ Chunking â†’ Embeddings â†’ Storage
```

---

## 1. FLUXO DE DOCUMENTOS (PDF/DOCX/TXT)

### ğŸ“¤ Etapa 1: Upload
- **Arquivo**: `app/api/documents/upload/route.ts`
- **Frontend**: Envia arquivo + `organizationId` via FormData
- **O que acontece**:
  - Recebe arquivo do frontend
  - Calcula hash SHA256 (evita duplicatas)
  - Salva arquivo em disco: `/public/uploads/{orgId}/{ano}/{mes}/{hash}-{nome}.{ext}`
  - Cria registro na tabela `documents` com status `pending`
- **Tabela BD**: `documents`
  - `document_type`: ENUM('pdf', 'docx', 'doc', 'txt', 'other')
  - `status`: ENUM('pending', 'processing', 'completed', 'failed')

**Exemplo de registro criado:**
```json
{
  "id": "uuid",
  "organizationId": "uuid",
  "fileName": "contrato.docx",
  "filePath": "/uploads/org-id/2025/12/abc123-contrato.docx",
  "fileSize": 45000,
  "documentType": "docx",
  "status": "pending"
}
```

---

### ğŸ”„ Etapa 2: ConversÃ£o para Markdown (NormalizaÃ§Ã£o)
- **Arquivo**: `lib/services/document-converter.ts`
- **Trigger**: Processamento assÃ­ncrono ou manual
- **O que faz**:
  - **PDF** â†’ Extrai texto usando `pdf-parse`, converte para Markdown preservando estrutura
  - **DOCX** â†’ Extrai texto formatado usando `mammoth`, converte para Markdown
  - **TXT** â†’ Converte diretamente para Markdown
- **ValidaÃ§Ãµes**:
  - Tamanho mÃ­nimo: 100 palavras
  - Tamanho mÃ¡ximo: 100.000 palavras
  - Filtra documentos muito pequenos ou muito grandes
- **Resultado**: String Markdown estruturado

**Exemplo de Markdown gerado:**
```markdown
# Contrato de PrestaÃ§Ã£o de ServiÃ§os

## Parte Contratante
Nome: Empresa XYZ Ltda
CNPJ: 12.345.678/0001-90

## Objeto
PrestaÃ§Ã£o de serviÃ§os de consultoria...
```

---

### ğŸ¤– Etapa 3: ClassificaÃ§Ã£o com IA
- **Arquivo**: `lib/services/classifier.ts`
- **Modelos suportados**: OpenAI GPT-4, Google Gemini
- **O que faz**:
  1. Carrega schema de classificaÃ§Ã£o da tabela `classification_configs` (por categoria)
  2. Monta prompt com o Markdown do documento
  3. Chama LLM com schema Zod dinÃ¢mico
  4. Extrai metadados estruturados (JSON)
- **Tabela BD**: `classification_configs`
  - `document_category`: ENUM('juridico', 'contabil', 'geral')
  - ContÃ©m schemas Zod customizÃ¡veis por categoria

**Exemplo de metadados extraÃ­dos:**
```json
{
  "title": "Contrato de PrestaÃ§Ã£o de ServiÃ§os - Empresa XYZ",
  "summary": "Contrato de consultoria com prazo de 12 meses...",
  "document_area": "contratual",
  "tags": ["contrato", "consultoria", "prestaÃ§Ã£o de serviÃ§os"],
  "entities": {
    "contratante": "Empresa XYZ Ltda",
    "cnpj": "12.345.678/0001-90"
  }
}
```

---

### âœ‚ï¸ Etapa 4: Chunking
- **Arquivo**: `lib/services/chunker.ts`
- **EstratÃ©gia**: Chunking semÃ¢ntico por seÃ§Ãµes
- **O que faz**:
  - Divide Markdown em chunks de **~800 tokens** (medidos com `tiktoken`)
  - Preserva contexto semÃ¢ntico (nÃ£o corta no meio de parÃ¡grafos/seÃ§Ãµes)
  - Prioriza quebras em headers (`##`), depois parÃ¡grafos vazios
  - Cada chunk recebe Ã­ndice sequencial
- **Resultado**: Array de objetos chunk

**Exemplo de chunk:**
```json
{
  "chunkIndex": 0,
  "content": "# Contrato de PrestaÃ§Ã£o de ServiÃ§os\n\n## Parte Contratante...",
  "tokenCount": 450,
  "startLine": 0,
  "endLine": 25
}
```

---

### ğŸ§  Etapa 5: Embeddings
- **Arquivo**: `lib/services/embedding-generator.ts`
- **Modelo**: OpenAI `text-embedding-3-small` (1536 dimensÃµes)
- **O que faz**:
  - Gera vetor de 1536 dimensÃµes para cada chunk
  - Trunca textos que excedem limite de tokens (8191)
  - Processa em batch para eficiÃªncia
  - Conta tokens precisos com `tiktoken`
- **Resultado**: Array de vetores `number[][]`

**Exemplo:**
```javascript
[
  [0.012, -0.034, 0.056, ...], // 1536 dimensÃµes
  [0.023, -0.045, 0.067, ...]
]
```

---

### ğŸ’¾ Etapa 6: Storage
- **Arquivo**: `lib/services/store-embeddings.ts`
- **O que salva**:
  1. **`document_files`**: Link para arquivo fÃ­sico + status
  2. **`templates`**: Documento classificado + metadados completos
  3. **`template_chunks`**: Chunks individuais + embeddings (pgvector)

**Estrutura final no BD:**
```
document_files (id: doc-123)
  â†“
templates (id: tpl-456, documentFileId: doc-123)
  â†“ metadata: {title, summary, tags, ...}
  â†“
template_chunks (templateId: tpl-456)
  â”œâ”€ chunk 0: "# Contrato..." + embedding[1536]
  â”œâ”€ chunk 1: "## ClÃ¡usulas..." + embedding[1536]
  â””â”€ chunk 2: "## VigÃªncia..." + embedding[1536]
```

---

## 2. FLUXO DE SPED (Arquivos ContÃ¡beis)

### ğŸ“¤ Etapa 1: Upload
- **Arquivo**: `app/api/sped/upload/route.ts`
- **Formato**: Arquivo `.txt` com estrutura SPED (pipes `|`)
- **O que acontece**:
  - Salva arquivo em disco
  - Cria registro na tabela `sped_files`
  - **Inicia processamento assÃ­ncrono** (pode demorar minutos)
  - Retorna `jobId` para tracking via SSE
- **Tabela BD**: `sped_files`
  - `file_type`: ENUM('ecd', 'ecf')
  - `status`: ENUM('pending', 'processing', 'completed', 'failed')

---

### ğŸ” Etapa 2: Parse (NormalizaÃ§Ã£o)
- **Arquivo**: `lib/services/sped-parser.ts`
- **O que faz**:
  - LÃª arquivo linha por linha
  - Identifica tipo de registro: `I010`, `I050`, `I200`, `J050`, `J100`, etc.
  - Extrai dados estruturados:
    - **Plano de Contas** (registro I050)
    - **Saldos ContÃ¡beis** (registro I200/I250)
    - **LanÃ§amentos** (registro J100/J150)
    - **Partidas** (registro J150 items)
- **Tabelas BD populadas**:
  - `chart_of_accounts`: Plano de contas completo
  - `account_balances`: Saldos iniciais/finais por conta
  - `journal_entries`: CabeÃ§alhos de lanÃ§amentos
  - `journal_items`: DÃ©bitos e crÃ©ditos de cada lanÃ§amento

**Exemplo de dados parseados:**
```json
{
  "plano_contas": [
    {"codigo": "1.01.01.01", "nome": "Caixa Geral", "tipo": "A"},
    {"codigo": "2.01.01.01", "nome": "Fornecedores", "tipo": "P"}
  ],
  "saldos": [
    {"conta": "1.01.01.01", "saldo_inicial": 50000, "saldo_final": 75000}
  ],
  "lancamentos": [
    {
      "numero": "LC-001",
      "data": "2024-01-15",
      "partidas": [
        {"conta": "1.01.01.01", "debito": 10000},
        {"conta": "3.01.01.01", "credito": 10000}
      ]
    }
  ]
}
```

---

### ğŸ“ Etapa 3: GeraÃ§Ã£o de Markdown Resumo
- **Arquivo**: `lib/services/sped-classifier.ts` â†’ `generateSpedSummaryMarkdown()`
- **O que gera**:
  - IdentificaÃ§Ã£o da empresa (CNPJ, nome, perÃ­odo)
  - EstatÃ­sticas gerais (nÂº de contas, saldos, lanÃ§amentos)
  - Amostra de contas principais (top 50)
  - Amostra de saldos (top 50)
- **Resultado**: Markdown estruturado otimizado para RAG

**Exemplo de Markdown gerado:**
```markdown
# Arquivo SPED ECD - Empresa XYZ Ltda

## IdentificaÃ§Ã£o
- **CNPJ:** 12.345.678/0001-90
- **Empresa:** Empresa XYZ Ltda
- **PerÃ­odo:** 2024-01-01 a 2024-12-31

## EstatÃ­sticas
- **Contas ContÃ¡beis:** 450
- **Saldos:** 450
- **LanÃ§amentos:** 12.543
- **Partidas:** 28.765

## Principais Contas
### Ativo
- 1.01.01.01 - Caixa Geral
- 1.01.02.01 - Bancos Conta Movimento
...
```

---

### âœ‚ï¸ Etapa 4: Chunking ContÃ¡bil
- **Arquivo**: `lib/services/accounting-chunker.ts`
- **Implementado em**: `lib/services/sped-rag-processor.ts`
- **EstratÃ©gia**: Chunking por conta contÃ¡bil
- **O que faz**:
  - Agrupa informaÃ§Ãµes **por conta contÃ¡bil**
  - Cada chunk contÃ©m:
    - Dados da conta (cÃ³digo, nome, tipo)
    - Saldo inicial e final
    - LanÃ§amentos relacionados (dÃ©bitos e crÃ©ditos)
  - MantÃ©m **contexto financeiro intacto**
  - Limite: ~800 tokens por chunk

**Exemplo de chunk contÃ¡bil:**
```markdown
## Conta: 1.01.01.01 - Caixa Geral

**Tipo:** Ativo
**Saldo Inicial:** R$ 50.000,00
**Saldo Final:** R$ 75.000,00

### LanÃ§amentos:
- 2024-01-15: D R$ 10.000,00 (LC-001 - Recebimento de vendas)
- 2024-02-20: C R$ 5.000,00 (LC-045 - Pagamento fornecedor)
...
```

---

### ğŸ§  Etapas 5 e 6: Embeddings + Storage
- **Mesmo processo de Documentos**
- Cada chunk contÃ¡bil vira um embedding
- Storage em: `document_files`, `templates`, `template_chunks`

---

## 3. FLUXO DE CSV

### ğŸ“¤ Etapa 1: Upload
- **Arquivo**: `app/api/csv/upload/route.ts`
- **Tabela BD**: `csv_imports`

---

### ğŸ” Etapa 2: Parse (NormalizaÃ§Ã£o)
- **Arquivo**: `lib/services/csv-parser.ts`
- **DetecÃ§Ã£o automÃ¡tica**:
  - **Delimiter**: Testa `,` e `;` e escolhe o mais comum
  - **Encoding**: Testa UTF-8, ISO-8859-1, Windows-1252
  - **Headers**: Detecta se primeira linha Ã© header
- **O que faz**:
  - Parse linha por linha
  - Normaliza tipos de dados (nÃºmero, texto, data)
  - Detecta colunas vazias/invÃ¡lidas
- **Tabela BD**: `csv_data` (dados brutos parseados, JSONB)

---

### ğŸ“Š Etapa 3: AnÃ¡lise EstatÃ­stica + Markdown
- **Arquivo**: `lib/services/csv-rag-processor.ts` â†’ `generateCsvAnalysisMarkdown()`
- **O que analisa**:
  - Estrutura: nÂº de colunas, tipos (numÃ©rico vs categÃ³rico)
  - EstatÃ­sticas por coluna: valores Ãºnicos, missing values, min/max
  - DistribuiÃ§Ãµes de dados
  - PossÃ­veis usos (financeiro, vendas, estoque)
- **O que gera**: Markdown com:
  - Resumo estatÃ­stico
  - Amostra das primeiras 50 linhas
  - Insights sobre os dados

**Exemplo de Markdown gerado:**
```markdown
# AnÃ¡lise CSV - vendas_2024.csv

## Estrutura
- **Total de Colunas:** 8
- **Colunas NumÃ©ricas:** 4 (valor, quantidade, desconto, total)
- **Colunas CategÃ³ricas:** 4 (data, produto, cliente, vendedor)
- **Total de Linhas:** 1.250

## EstatÃ­sticas por Coluna

### valor
- **Tipo:** NumÃ©rico
- **Valores Ãšnicos:** 487
- **Min:** R$ 10,00
- **Max:** R$ 15.000,00
- **MÃ©dia:** R$ 1.250,00

### produto
- **Tipo:** CategÃ³rico
- **Valores Ãšnicos:** 45
- **Mais Comum:** Produto A (230 ocorrÃªncias)

## Amostra de Dados (primeiras 10 linhas)
| data       | produto   | cliente | valor    |
|------------|-----------|---------|----------|
| 2024-01-05 | Produto A | CLI-001 | 1.250,00 |
| 2024-01-05 | Produto B | CLI-002 | 850,00   |
...

## PossÃ­veis Usos
- AnÃ¡lise financeira ou vendas
- AnÃ¡lise temporal/sÃ©ries temporais
```

---

### ğŸ¤– Etapa 4: ClassificaÃ§Ã£o com IA
- **Mesmo processo de Documentos**
- **O que classifica**:
  - Tipo de dados (financeiro, vendas, estoque, RH, etc)
  - Qualidade dos dados (completos, com missing values, etc)
  - SugestÃµes de uso

---

### âœ‚ï¸ Etapa 5: Chunking Inteligente
- **Arquivo**: `lib/services/csv-rag-processor.ts`
- **EstratÃ©gia**: Chunking por grupos de linhas
- **O que faz**:
  - Divide por grupos de **50-100 linhas**
  - Cada chunk mantÃ©m **header** para contexto
  - Se houver coluna categÃ³rica importante, pode dividir por categoria
  - Cada chunk = header + N linhas + estatÃ­sticas do grupo

**Exemplo de chunk CSV:**
```markdown
## Vendas - PerÃ­odo 2024-01-01 a 2024-01-15 (Linhas 1-50)

| data       | produto   | cliente | valor    |
|------------|-----------|---------|----------|
| 2024-01-05 | Produto A | CLI-001 | 1.250,00 |
...

**EstatÃ­sticas do PerÃ­odo:**
- Total de Vendas: R$ 45.000,00
- Ticket MÃ©dio: R$ 900,00
- Produto Mais Vendido: Produto A
```

---

### ğŸ§  Etapas 6 e 7: Embeddings + Storage
- **Mesmo processo de Documentos**

---

## Estrutura do Banco de Dados

### ğŸ“ Tabelas de Arquivo Bruto (por tipo)
```sql
documents          -- Documentos gerais (PDF/DOCX/TXT)
  â”œâ”€ id, fileName, filePath, fileSize, fileHash
  â”œâ”€ documentType: ENUM('pdf', 'docx', 'doc', 'txt')
  â”œâ”€ status: ENUM('pending', 'processing', 'completed', 'failed')
  â””â”€ organizationId, uploadedBy

sped_files         -- Arquivos SPED
  â”œâ”€ id, fileName, filePath, fileHash
  â”œâ”€ fileType: ENUM('ecd', 'ecf')
  â”œâ”€ cnpj, companyName, periodStart, periodEnd
  â”œâ”€ status: ENUM('pending', 'processing', 'completed', 'failed')
  â””â”€ organizationId, uploadedBy

csv_imports        -- Arquivos CSV
  â”œâ”€ id, fileName, filePath, fileHash
  â”œâ”€ delimiter, encoding, hasHeader
  â”œâ”€ rowCount, columnCount
  â”œâ”€ status: ENUM('pending', 'processing', 'completed', 'failed')
  â””â”€ organizationId, uploadedBy
```

### ğŸ“Š Tabelas de Dados Parseados (especÃ­ficas por tipo)
```sql
-- SPED especÃ­fico
chart_of_accounts  -- Plano de contas
account_balances   -- Saldos contÃ¡beis
journal_entries    -- LanÃ§amentos contÃ¡beis
journal_items      -- Partidas dos lanÃ§amentos

-- CSV especÃ­fico
csv_data           -- Dados parseados (JSONB)
```

### ğŸ§  Tabelas RAG (UNIFICADAS - todos os formatos)
```sql
document_files           -- Arquivo fÃ­sico + tipo + status
  â”œâ”€ id, filePath, mimeType
  â”œâ”€ fileType: ENUM('document', 'sped', 'csv')
  â”œâ”€ originalId: UUID (aponta para documents/sped_files/csv_imports)
  â””â”€ status: ENUM('pending', 'processing', 'completed', 'failed')

classification_configs   -- Schemas de classificaÃ§Ã£o
  â”œâ”€ id, name, documentCategory
  â”œâ”€ systemPrompt, modelProvider
  â””â”€ extractionFunctionCode (schema Zod)

template_schema_configs  -- Schemas de template
  â”œâ”€ id, name, documentCategory
  â””â”€ fields (JSONB com definiÃ§Ãµes de campos)

templates                -- Documento classificado
  â”œâ”€ id, documentFileId
  â”œâ”€ title, markdown
  â”œâ”€ metadata (JSONB com dados classificados)
  â”œâ”€ costUsd, tokensUsed
  â””â”€ organizationId, createdBy

template_chunks          -- Chunks + embeddings
  â”œâ”€ id, templateId
  â”œâ”€ chunkIndex, content
  â”œâ”€ embedding: vector(1536)  â† pgvector
  â”œâ”€ tokenCount
  â””â”€ metadata (JSONB)
```

### ğŸ”— Relacionamentos
```
documents â†’ document_files â†’ templates â†’ template_chunks
                                â†“
                           embedding (vector)

sped_files â†’ document_files â†’ templates â†’ template_chunks
    â†“
chart_of_accounts, account_balances, journal_entries

csv_imports â†’ document_files â†’ templates â†’ template_chunks
    â†“
csv_data
```

---

## Fluxo Visual Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        UPLOAD                               â”‚
â”‚  Frontend â†’ API â†’ Salva Disco â†’ Cria Registro BD           â”‚
â”‚                                                             â”‚
â”‚  Documentos: documents                                      â”‚
â”‚  SPED:       sped_files                                     â”‚
â”‚  CSV:        csv_imports                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NORMALIZAÃ‡ÃƒO                             â”‚
â”‚  Converte para formato comum (Markdown)                    â”‚
â”‚                                                             â”‚
â”‚  Documentos: PDFâ†’MD, DOCXâ†’MD, TXTâ†’MD                       â”‚
â”‚  SPED:       Parse â†’ Resumo Markdown                       â”‚
â”‚  CSV:        Parse â†’ AnÃ¡lise Markdown                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CLASSIFICAÃ‡ÃƒO                             â”‚
â”‚  IA extrai metadados estruturados                          â”‚
â”‚                                                             â”‚
â”‚  LLM (GPT-4/Gemini) + Schema Zod                          â”‚
â”‚  â†’ {title, summary, area, tags, entities, ...}            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CHUNKING                                â”‚
â”‚  Divide em pedaÃ§os menores (~800 tokens)                   â”‚
â”‚                                                             â”‚
â”‚  Documentos: Por semÃ¢ntica (headers, parÃ¡grafos)           â”‚
â”‚  SPED:       Por conta contÃ¡bil                            â”‚
â”‚  CSV:        Por grupo de linhas (50-100)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDINGS                               â”‚
â”‚  Gera vetores (1536 dims) para cada chunk                  â”‚
â”‚                                                             â”‚
â”‚  OpenAI text-embedding-3-small                             â”‚
â”‚  â†’ [0.012, -0.034, 0.056, ...]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STORAGE                                 â”‚
â”‚  Salva tudo no PostgreSQL + pgvector                       â”‚
â”‚                                                             â”‚
â”‚  document_files â†’ templates â†’ template_chunks              â”‚
â”‚                                   â†“                         â”‚
â”‚                              embedding: vector(1536)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAG QUERY                                 â”‚
â”‚  Busca semÃ¢ntica usando vetores                            â”‚
â”‚                                                             â”‚
â”‚  User Query â†’ Embedding â†’ Cosine Similarity â†’ Top Results  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## DiferenÃ§as Entre os Formatos

### Por que cada formato Ã© diferente?

| Aspecto | Documentos | SPED | CSV |
|---------|-----------|------|-----|
| **Estrutura Original** | Texto corrido/formatado | Linhas com pipes (estruturado) | Tabela (linhas x colunas) |
| **NormalizaÃ§Ã£o** | ConversÃ£o para Markdown | Parse de registros â†’ Markdown resumo | Parse + anÃ¡lise â†’ Markdown |
| **Chunking** | SemÃ¢ntico (por seÃ§Ãµes) | Por conta contÃ¡bil | Por grupos de linhas |
| **Objetivo** | Busca por conceitos | Busca por contas/lanÃ§amentos | Busca por dados tabulares |
| **Tabelas BD EspecÃ­ficas** | Nenhuma (sÃ³ documento) | chart_of_accounts, balances, etc | csv_data |
| **Tempo de Processamento** | RÃ¡pido (segundos) | Lento (minutos) | MÃ©dio (segundos a minutos) |

---

## Estado Atual do Sistema

### âœ… O que estÃ¡ implementado e funcionando:
- âœ… Fluxo completo de Documentos (upload â†’ RAG â†’ query)
- âœ… Upload + Parse de SPED
- âœ… Upload + Parse de CSV
- âœ… SPED RAG processor (cÃ³digo implementado)
- âœ… CSV RAG processor (cÃ³digo implementado)
- âœ… Dashboard unificado de status
- âœ… Testes automatizados
- âœ… DocumentaÃ§Ã£o de usuÃ¡rio

### âš ï¸ O que estÃ¡ implementado mas NÃƒO testado:
- âš ï¸ SPED RAG end-to-end (chunking + embeddings)
- âš ï¸ CSV RAG end-to-end (chunking + embeddings)
- âš ï¸ Upload de DOCX (teve problemas de ENUM, foi corrigido mas nÃ£o testado)

### âŒ O que estÃ¡ quebrado/inconsistente:
- âŒ Tabela `documents`: coluna `document_type` foi deletada e recriada
  - Pode ter dados inconsistentes
  - Registros antigos podem ter `document_type = NULL` ou 'other'
- âŒ Banco de dados pode ter registros Ã³rfÃ£os de migraÃ§Ãµes anteriores

### â“ O que NÃƒO sabemos (precisa testar):
- â“ Upload de DOCX funciona apÃ³s fix de ENUM?
- â“ SPED gera chunks + embeddings corretamente?
- â“ CSV gera chunks + embeddings corretamente?
- â“ Busca vetorial (RAG query) retorna resultados relevantes?
- â“ Performance com arquivos grandes (10MB+, 100k+ linhas)?

---

## PrÃ³ximos Passos (Plano de ValidaÃ§Ã£o)

### 1. Validar Banco de Dados
```sql
-- Verificar ENUMs
SELECT typname, enumlabel 
FROM pg_enum 
JOIN pg_type ON pg_enum.enumtypid = pg_type.oid 
WHERE typname LIKE '%document%';

-- Verificar coluna document_type existe
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_name = 'documents' 
AND column_name = 'document_type';

-- Contar registros por status
SELECT status, COUNT(*) 
FROM documents 
GROUP BY status;

SELECT status, COUNT(*) 
FROM sped_files 
GROUP BY status;

SELECT status, COUNT(*) 
FROM csv_imports 
GROUP BY status;
```

### 2. Testar Upload de DOCX
1. Hard refresh no navegador (Cmd+Shift+R)
2. Upload de 1 arquivo DOCX pequeno (~50KB)
3. Verificar logs do servidor
4. Confirmar registro em `documents` com `status = 'pending'`
5. Triggerar processamento manual

### 3. Testar Processamento RAG
1. Para cada formato (DOC, SPED, CSV):
   - Fazer upload
   - Aguardar processamento completo
   - Verificar `templates` criado
   - Verificar `template_chunks` com embeddings
   - Contar chunks gerados

### 4. ValidaÃ§Ã£o End-to-End
1. Upload de 1 arquivo de cada tipo
2. Aguardar processamento completo (status='completed')
3. Fazer query RAG: "resumo do contrato" / "saldo da conta caixa" / "total de vendas"
4. Verificar se chunks retornados sÃ£o relevantes
5. Medir tempo de resposta

### 5. Testes de Performance
- Arquivo DOCX grande (5MB+)
- Arquivo SPED grande (100k+ linhas)
- Arquivo CSV grande (50k+ linhas)
- Medir tempo de processamento
- Verificar uso de memÃ³ria
- Verificar nÃºmero de chunks gerados

---

## GlossÃ¡rio

- **NormalizaÃ§Ã£o**: ConversÃ£o de diferentes formatos (PDF, SPED, CSV) para formato comum (Markdown)
- **Chunking**: DivisÃ£o de documento em pedaÃ§os menores (~800 tokens)
- **Embedding**: Vetor numÃ©rico (1536 dimensÃµes) que representa semanticamente um chunk
- **Template**: Documento normalizado + classificado + metadados
- **RAG** (Retrieval-Augmented Generation): Busca semÃ¢ntica + geraÃ§Ã£o de resposta com LLM
- **pgvector**: ExtensÃ£o PostgreSQL para armazenar e buscar vetores
- **Cosine Similarity**: MÃ©trica de similaridade entre vetores (usado na busca)

---

## ReferÃªncias

### Arquivos Principais
- Upload: `app/api/documents/upload/route.ts`, `app/api/sped/upload/route.ts`, `app/api/csv/upload/route.ts`
- NormalizaÃ§Ã£o: `lib/services/document-converter.ts`, `lib/services/sped-parser.ts`, `lib/services/csv-parser.ts`
- ClassificaÃ§Ã£o: `lib/services/classifier.ts`, `lib/services/sped-classifier.ts`
- Chunking: `lib/services/chunker.ts`, `lib/services/accounting-chunker.ts`
- Embeddings: `lib/services/embedding-generator.ts`
- Storage: `lib/services/store-embeddings.ts`
- RAG Processors: `lib/services/rag-processor.ts`, `lib/services/sped-rag-processor.ts`, `lib/services/csv-rag-processor.ts`

### Schemas BD
- Documentos: `lib/db/schema/documents.ts`
- SPED: `lib/db/schema/sped.ts`
- CSV: `lib/db/schema/csv.ts`
- RAG: `lib/db/schema/rag.ts`

